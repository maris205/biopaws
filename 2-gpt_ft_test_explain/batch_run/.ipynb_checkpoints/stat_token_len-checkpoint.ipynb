{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9561c7-9f13-4f35-bc4c-6751cdbcb5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # 设置环境变量\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# # 打印环境变量以确认设置成功\n",
    "# print(os.environ.get('HF_ENDPOINT'))\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039873dc-8712-49e5-9d46-3cb546875860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 分析子集: protein_pair_short ===\n",
      "总序列数: 40000\n",
      "氨基酸长度统计（近似，token数 ≈ aa数 × 1.5）:\n",
      "  Min: 21\n",
      "  25%: 63\n",
      "  50% (中位数): 88\n",
      "  75%: 114\n",
      "  90%: 132\n",
      "  95%: 140\n",
      "  99%: 148\n",
      "  Max: 169\n",
      "  保留 90% 序列完整 → 建议 max_length = 142\n",
      "  保留 95% 序列完整 → 建议 max_length = 150\n",
      "  保留 99% 序列完整 → 建议 max_length = 158\n",
      "\n",
      "=== 分析子集: protein_pair_full ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since dnagpt/biopaws couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'protein_pair_full' at /root/.cache/huggingface/datasets/dnagpt___biopaws/protein_pair_full/0.0.0/04f23788fc267a3ab45cd27f39e8d6566a412a41 (last modified on Tue Dec 30 20:40:55 2025).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1283 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总序列数: 40000\n",
      "氨基酸长度统计（近似，token数 ≈ aa数 × 1.5）:\n",
      "  Min: 15\n",
      "  25%: 87\n",
      "  50% (中位数): 142\n",
      "  75%: 219\n",
      "  90%: 310\n",
      "  95%: 400\n",
      "  99%: 675\n",
      "  Max: 4516\n",
      "  保留 90% 序列完整 → 建议 max_length = 320\n",
      "  保留 95% 序列完整 → 建议 max_length = 410\n",
      "  保留 99% 序列完整 → 建议 max_length = 685\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# ==================== 配置 ====================\n",
    "MODEL_NAME = \"gpt2\"  # 你用的 tokenizer\n",
    "DATASET_NAME = \"dnagpt/biopaws\"\n",
    "\n",
    "SUBSETS = [\"protein_pair_short\", \"protein_pair_full\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def analyze_length(subset_name):\n",
    "    print(f\"\\n=== 分析子集: {subset_name} ===\")\n",
    "    dataset = load_dataset(DATASET_NAME, subset_name)[\"train\"]\n",
    "    \n",
    "    # 计算所有序列的 token 长度\n",
    "    lengths1 = []\n",
    "    lengths2 = []\n",
    "    for item in dataset:\n",
    "        len1 = len(tokenizer.encode(item[\"sentence1\"], add_special_tokens=False))\n",
    "        len2 = len(tokenizer.encode(item[\"sentence2\"], add_special_tokens=False))\n",
    "        lengths1.append(len1)\n",
    "        lengths2.append(len2)\n",
    "    \n",
    "    all_lengths = lengths1 + lengths2\n",
    "    all_lengths = np.array(all_lengths)\n",
    "    \n",
    "    print(f\"总序列数: {len(all_lengths)}\")\n",
    "    print(f\"氨基酸长度统计（近似，token数 ≈ aa数 × 1.5）:\")\n",
    "    print(f\"  Min: {all_lengths.min()}\")\n",
    "    print(f\"  25%: {np.percentile(all_lengths, 25):.0f}\")\n",
    "    print(f\"  50% (中位数): {np.percentile(all_lengths, 50):.0f}\")\n",
    "    print(f\"  75%: {np.percentile(all_lengths, 75):.0f}\")\n",
    "    print(f\"  90%: {np.percentile(all_lengths, 90):.0f}\")\n",
    "    print(f\"  95%: {np.percentile(all_lengths, 95):.0f}\")\n",
    "    print(f\"  99%: {np.percentile(all_lengths, 99):.0f}\")\n",
    "    print(f\"  Max: {all_lengths.max()}\")\n",
    "    \n",
    "    # 推荐 max_length（保留 95% 或 99% 序列不截断）\n",
    "    for perc in [90, 95, 99]:\n",
    "        val = np.percentile(all_lengths, perc)\n",
    "        print(f\"  保留 {perc}% 序列完整 → 建议 max_length = {int(val) + 10}\")  # +10 留余量\n",
    "    \n",
    "    return all_lengths\n",
    "\n",
    "# 运行统计\n",
    "for subset in SUBSETS:\n",
    "    analyze_length(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77b16f-b571-4684-8da5-365c3bb8d34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
