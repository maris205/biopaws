#!/usr/bin/env python
# coding: utf-8

# In[1]:


# import os

# # è®¾ç½®ç¯å¢ƒå˜é‡
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# # æ‰“å°ç¯å¢ƒå˜é‡ä»¥ç¡®è®¤è®¾ç½®æˆåŠŸ
# print(os.environ.get('HF_ENDPOINT'))

# import subprocess
# import os

# result = subprocess.run('bash -c "source /etc/network_turbo && env | grep proxy"', shell=True, capture_output=True, text=True)
# output = result.stdout
# for line in output.splitlines():
#     if '=' in line:
#         var, value = line.split('=', 1)
#         os.environ[var] = value


# In[2]:


import os
import subprocess
import json
import random
import re
import torch  # æ–°å¢
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig # æ–°å¢
from huggingface_hub import login # æ–°å¢
from sklearn.metrics import accuracy_score, classification_report


# In[3]:


# ==========================================
# 3. åŠ è½½æ¨¡å‹ (Base Model + LoRA Adapter)
# ==========================================
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel # ğŸ‘ˆ å¿…é¡»å¼•å…¥è¿™ä¸ªåº“
from huggingface_hub import login

# å®šä¹‰è·¯å¾„
BASE_MODEL_ID = "meta-llama/Llama-3.1-8B"       # åŸå§‹åº•åº§
ADAPTER_PATH = "./Llama-3.1-8B-PAWS-En-Finetuned" # ä½ åˆšè®­ç»ƒå¥½çš„å¾®è°ƒç»“æœ
HF_TOKEN = ""

print(f"Logging in...")
login(token=HF_TOKEN)


# In[ ]:


print(f"Loading Base Model: {BASE_MODEL_ID}...")
# 1. å…ˆåŠ è½½åº•åº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    torch_dtype=torch.float16, # 4090 ä¸Šæ¨ç†æ¨èç”¨åŠç²¾åº¦
    device_map="auto",
)

print(f"Loading LoRA Adapter: {ADAPTER_PATH}...")
# 2. å…³é”®æ­¥éª¤ï¼šæŠŠå¾®è°ƒçš„è¡¥ä¸â€œæŒ‚â€åˆ°åº•åº§ä¸Š
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)

print("Loading Tokenizer...")
# 3. åŠ è½½åˆ†è¯å™¨ (é€šå¸¸ç›´æ¥ç”¨åº•åº§çš„ï¼Œæˆ–è€…ä½ ä¿å­˜ç›®å½•é‡Œçš„)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)

# è®¾å®š Llama 3 çš„ç»ˆæ­¢ç¬¦
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

print("Model loaded successfully! Ready for inference.")


# In[9]:


# ==========================================
# 2. æ•°æ®å‡†å¤‡ (åŠ è½½ã€é‡‡æ ·ã€æ‰“ä¹±)
# ==========================================
print("Loading dataset...")
try:
    # å°è¯•åŠ è½½æ•°æ®é›†
    protein_data = load_dataset('dnagpt/biopaws', 'protein_pair_short')
    # local_dataset_path = "./biopaws" 
    # # ä¿®æ”¹è¿™é‡Œï¼šç¬¬ä¸€ä¸ªå‚æ•°æ”¹ä¸ºæœ¬åœ°è·¯å¾„
    # protein_data = load_dataset(
    #     local_dataset_path,          # ğŸ‘ˆ è¿™é‡Œæ”¹æˆä½ çš„æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„
    #     'protein_sim_pair_450bp',    # é…ç½®åä¿æŒä¸å˜
    #     trust_remote_code=True       # ğŸ‘ˆ åŠ ä¸Šè¿™ä¸ªï¼Œå…è®¸æ‰§è¡Œæœ¬åœ°æ–‡ä»¶å¤¹é‡Œçš„åŠ è½½è„šæœ¬
    # )

    # æ–¹æ³•ï¼šç›´æ¥ä» CSV æ–‡ä»¶åŠ è½½ï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰
    # å‡è®¾ä½ çš„ CSV æ–‡ä»¶æœ‰ä¸‰åˆ—ï¼šsentence1, sentence2, label
    #csv_path = "protein_pair_sample_200.csv" #protein_pair_sample_200_length_restricted.csv  protein_pair_sample_200.csv 
    
    # ä½¿ç”¨ load_dataset ç›´æ¥è¯»å–æœ¬åœ° CSV
    #protein_data = load_dataset("csv", data_files=csv_path)
    
    ds = protein_data['train']
    
    # åˆ†ç¦»æ•°æ®
    data_label_0 = [item for item in ds if item['label'] == 0]
    data_label_1 = [item for item in ds if item['label'] == 1]
    
    # éšæœºé‡‡æ · (å„30%)
    random.seed(42)
    sample_num_0 = int(len(data_label_0) * 0.3)
    sample_num_1 = int(len(data_label_1) * 0.3)
    sampled_0 = random.sample(data_label_0, sample_num_0)
    sampled_1 = random.sample(data_label_1, sample_num_1)
    
    # åˆå¹¶å¹¶æ‰“ä¹±
    combined_data = sampled_0 + sampled_1
    random.shuffle(combined_data)
    
    print(f"Data prepared: {len(combined_data)} pairs.")

except Exception as e:
    print(f"Error loading dataset: {e}")
    
# æ„å»ºç”¨äº Prompt çš„ JSON Listï¼Œå¹¶å»ºç«‹ ID -> Label çš„æ˜ å°„ç”¨äºåç»­éªŒè¯
prompt_data_list = []
id_to_ground_truth = {}

for idx, item in enumerate(combined_data, 1):
    prompt_data_list.append({
        "id": idx,
        "seq_a": item['sentence1'],
        "seq_b": item['sentence2']
    })
    id_to_ground_truth[idx] = item['label']




# ==========================================
# 3. æ„å»º Prompt (System vs User)
# ==========================================

# System Prompt: å®šä¹‰è§„åˆ™ã€ç±»æ¯”å’Œè¾“å‡ºæ ¼å¼
system_prompt = """You are an expert bioinformatics assistant capable of linguistic transfer learning.

1. The Concept:
In English, a sentence can be rearranged structurally but keep the same meaning (Paraphrase). Or, it can be scrambled to lose its logic (Adversarial).

2. The Analogy:
* **Homologous Proteins** are like **Paraphrases**: They have different sequences due to evolution, but they fold into the same structure and function.
* **Non-Homologous/Random Proteins** are like **Adversarial Sentences**: They look like proteins, but their internal structural logic is broken.

3. Output Requirements:
* I will provide a JSON list of protein pairs.
* You must return a RAW JSON object containing a list of results.
* The format must be strictly: `[{"id": 1, "prediction": "Homologous"}, {"id": 2, "prediction": "Non-Homologous"}, ...]`
* Do NOT provide explanations. Just the JSON array.
"""


# In[12]:


# ==========================================
# 5. è§£æç»“æœä¸è¯„ä¼° (ç§»åŠ¨åˆ°è¿™é‡Œï¼Œä»¥ä¾¿åœ¨å¾ªç¯ä¸­ä½¿ç”¨)
# ==========================================
def parse_llm_json(text):
    """æå–å¹¶è§£æ JSON"""
    try:
        # å¯»æ‰¾ JSON æ•°ç»„ [...]
        match = re.search(r"\[.*\]", text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        return json.loads(text)
    except Exception:
        return []


# ==========================================
# 4. æ‰§è¡Œæœ¬åœ°æ¨ç† (é’ˆå¯¹ Base æ¨¡å‹çš„ä¿®æ”¹ç‰ˆ)
# ==========================================
print("-" * 30)
print(f"Running inference ...")

# åˆå§‹åŒ–æ€»ä½“é¢„æµ‹åˆ—è¡¨
predictions_list = []

# æ‰¹æ¬¡å¤§å°ï¼šæœ€å¤š20ä¸ªåºåˆ—å¯¹ï¼ˆ10+10ï¼‰
batch_size = 10 #è¿™ä¸ªå¤ªå°æ•ˆæœä¹Ÿä¸å¥½ï¼Œä¸ºå•¥å‘¢ï¼Ÿ

# è®¡ç®—æ‰¹æ¬¡æ•°
num_batches = (len(prompt_data_list) + batch_size - 1) // batch_size

for batch_idx in range(num_batches):
    start = batch_idx * batch_size
    end = start + batch_size
    prompt_data_batch = prompt_data_list[start:end]
    
    if not prompt_data_batch:
        continue
    
    print(f"Processing batch {batch_idx + 1}/{num_batches} with {len(prompt_data_batch)} pairs...")
    
    # User Prompt: æä¾›å…·ä½“æ•°æ®ï¼ˆåŠ¨æ€æ•°é‡ï¼‰
    user_prompt = f"""Here is the JSON list of {len(prompt_data_batch)} protein pairs to analyze.
Using your intuition about "sequence syntax" and "structural integrity," determine if each pair is "Homologous" or "Non-Homologous".

Data:
{json.dumps(prompt_data_batch, indent=2)}
"""
    
    # --- å…³é”®ä¿®æ”¹ 1: æ‰‹åŠ¨æ‹¼æ¥ Promptï¼Œä¸ç”¨ Chat Template ---
    # Base æ¨¡å‹éœ€è¦ä½ åƒå†™æ–‡ç« å¼€å¤´ä¸€æ ·å¼•å¯¼å®ƒ
    # æˆ‘ä»¬åœ¨æœ€åå¼ºè¡ŒåŠ ä¸€ä¸ª ```json\n[ï¼Œè¯±å¯¼å®ƒç›´æ¥å¼€å§‹å†™ JSON æ•°ç»„
    raw_prompt = f"""
{system_prompt}

{user_prompt}

The results in JSON format are:
```json
[
"""

    # --- å…³é”®ä¿®æ”¹ 2: ç›´æ¥ç¼–ç å­—ç¬¦ä¸² ---
    input_ids = tokenizer(
        raw_prompt, 
        return_tensors="pt"
    ).input_ids.to(model.device)

    print("Generating response for batch...")

    try:
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_new_tokens=4096,
                # --- å…³é”®ä¿®æ”¹ 3: Base æ¨¡å‹åªè¦é‡åˆ° EOS å°±åœæ­¢ ---
                eos_token_id=tokenizer.eos_token_id, 
                pad_token_id=tokenizer.eos_token_id,
                do_sample=True,
                temperature=0.1,
                top_p=0.9,
            )

        # è§£ç 
        response = outputs[0][input_ids.shape[-1]:]
        full_content = tokenizer.decode(response, skip_special_tokens=True)
        
        # --- å…³é”®ä¿®æ”¹ 4: å› ä¸ºæˆ‘ä»¬åœ¨ Prompt é‡Œæ‰‹åŠ¨åŠ äº†å¼€å¤´ï¼Œè¿™é‡Œè¦è¡¥å›æ¥ ---
        # è¿™æ ·åç»­çš„ JSON è§£æå™¨æ‰èƒ½è¯»æ‡‚
        full_content = "[\n" + full_content 

        print("Response received for batch.")
        print(f"Response snippet: {full_content[:200]}...")

        # è§£ææ‰¹æ¬¡é¢„æµ‹
        predictions_batch = parse_llm_json(full_content)
        predictions_list.extend(predictions_batch)

    except Exception as e:
        print(f"Inference Failed for batch {batch_idx + 1}: {e}")



# æ˜ å°„ä¸è®¡ç®—
label_map = {"Homologous": 1, "Non-Homologous": 0}
y_true = []
y_pred = []

print("-" * 30)
if not predictions_list:
    print("Failed to parse JSON from model response.")
else:
    print(f"Parsed {len(predictions_list)} predictions.")
    
    for item in predictions_list:
        p_id = item.get('id')
        p_str = item.get('prediction') 
        
        # ç¡®ä¿ ID å­˜åœ¨ä¸”é¢„æµ‹å€¼æœ‰æ•ˆ
        if p_id in id_to_ground_truth and p_str in label_map:
            y_true.append(id_to_ground_truth[p_id])
            y_pred.append(label_map[p_str])

    # è¾“å‡ºæœ€ç»ˆæŒ‡æ ‡
    if y_true:
        acc = accuracy_score(y_true, y_pred)
        print(f"\nFinal Accuracy: {acc:.2%}")
        print("\nClassification Report:")
        print(classification_report(y_true, y_pred, target_names=["Non-Homologous (0)", "Homologous (1)"]))
        
        # ä¿å­˜ç»“æœç”¨äºåç»­åˆ†æ
        result_log = {
            "model": "llama3.1",
            "accuracy": acc,
            "predictions": predictions_list
        }
        print(acc)
        # with open("doubao_result.json", "w") as f:
        #     json.dump(result_log, f, indent=2)
        #     print("Results saved to 'doubao_result.json'")
    else:
        print("No valid matching IDs found between Prompt and Response.")


# In[ ]:


"""
------------------------------
Parsed 900 predictions.

Final Accuracy: 72.00%

Classification Report:
                    precision    recall  f1-score   support

Non-Homologous (0)       0.67      0.86      0.75       450
    Homologous (1)       0.81      0.58      0.67       450

          accuracy                           0.72       900
         macro avg       0.74      0.72      0.71       900
      weighted avg       0.74      0.72      0.71       900

0.72

"""