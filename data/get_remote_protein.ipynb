{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acaa7453-aa81-483c-8a98-8d98ba29855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # 设置环境变量\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# # 打印环境变量以确认设置成功\n",
    "# print(os.environ.get('HF_ENDPOINT'))\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd44c1c-8414-47b6-ba89-fdc5db3c5c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 filtered-SCOPe-2.08 数据集...\n",
      "总序列数: 14535\n",
      "示例数据字段: dict_keys(['id', 'primary', 'protein_length', 'class', 'fold', 'super_family', 'family', 'description'])\n",
      "示例数据: {'id': 'd1dlwa_', 'primary': 'slfeqlggqaavqavtaqfyaniqadatvatffngidmpnqtnktaaflcaalggpnawtgrnlkevhanmgvsnaqfttvighlrsaltgagvaaalveqtvavaetvrgdvvtv', 'protein_length': 116, 'class': 'a', 'fold': 'a.1', 'super_family': 'a.1.1', 'family': 'a.1.1.1', 'description': 'd1dlwa_ a.1.1.1 (A:) Protozoan/bacterial hemoglobin {Ciliate (Paramecium caudatum) [TaxId: 5885]}'}\n",
      "有效 superfamily 数量: 1754\n",
      "构建正样本（同一 superfamily，不同 family）...\n",
      "正样本数量: 179702\n",
      "构建负样本（不同 superfamily）...\n",
      "负样本数量: 179702\n",
      "最终远程同源测试集规模: 359404 条\n",
      "  正样本 (同超家族不同家族): 179702\n",
      "  负样本 (不同超家族): 179702\n",
      "测试集已保存为 protein_pair_remote.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "# ==================== 步骤1：加载数据集 ====================\n",
    "print(\"正在加载 filtered-SCOPe-2.08 数据集...\")\n",
    "dataset = load_dataset(\"vkarthik095/filtered-SCOPe-2.08\")[\"train\"]\n",
    "\n",
    "print(f\"总序列数: {len(dataset)}\")\n",
    "print(\"示例数据字段:\", dataset[0].keys())\n",
    "print(\"示例数据:\", dataset[0])\n",
    "\n",
    "# ==================== 步骤2：按 superfamily 分组 ====================\n",
    "superfamily_to_items = {}\n",
    "\n",
    "for item in dataset:\n",
    "    seq = item[\"primary\"]  # 关键修正：用 'primary' 而不是 'sequence'\n",
    "    superfamily = item[\"super_family\"]  # e.g., \"a.1.1\"\n",
    "    family = item[\"family\"]             # e.g., \"a.1.1.1\"\n",
    "    \n",
    "    # 可选：长度过滤，与你的 short 数据集对齐\n",
    "    if not (40 <= len(seq) <= 250):\n",
    "        continue\n",
    "    \n",
    "    key = (superfamily, family)\n",
    "    if superfamily not in superfamily_to_items:\n",
    "        superfamily_to_items[superfamily] = {}\n",
    "    if key not in superfamily_to_items[superfamily]:\n",
    "        superfamily_to_items[superfamily][key] = []\n",
    "    superfamily_to_items[superfamily][key].append(seq)\n",
    "\n",
    "print(f\"有效 superfamily 数量: {len(superfamily_to_items)}\")\n",
    "\n",
    "# ==================== 步骤3：构建正样本（同一 superfamily，不同 family） ====================\n",
    "pos_pairs = []\n",
    "print(\"构建正样本（同一 superfamily，不同 family）...\")\n",
    "for superfamily, family_dict in superfamily_to_items.items():\n",
    "    families = list(family_dict.keys())\n",
    "    if len(families) < 2:\n",
    "        continue  # 需要至少两个不同 family\n",
    "    \n",
    "    # 不同 family 之间两两组合\n",
    "    for i in range(len(families)):\n",
    "        for j in range(i + 1, len(families)):\n",
    "            seqs1 = family_dict[families[i]]\n",
    "            seqs2 = family_dict[families[j]]\n",
    "            for s1 in seqs1:\n",
    "                for s2 in seqs2:\n",
    "                    pos_pairs.append({\n",
    "                        \"sentence1\": s1,\n",
    "                        \"sentence2\": s2,\n",
    "                        \"label\": 1\n",
    "                    })\n",
    "\n",
    "print(f\"正样本数量: {len(pos_pairs)}\")\n",
    "\n",
    "# ==================== 步骤4：构建负样本（不同 superfamily） ====================\n",
    "neg_pairs = []\n",
    "print(\"构建负样本（不同 superfamily）...\")\n",
    "all_seqs = [item[\"primary\"] for item in dataset if 40 <= len(item[\"primary\"]) <= 250]\n",
    "\n",
    "superfamilies = list(superfamily_to_items.keys())\n",
    "target_neg = len(pos_pairs)  # 平衡正负\n",
    "\n",
    "count = 0\n",
    "while count < target_neg:\n",
    "    sf1, sf2 = random.sample(superfamilies, 2)\n",
    "    # 从 sf1 和 sf2 各随机取一个序列\n",
    "    fam1 = random.choice(list(superfamily_to_items[sf1].keys()))\n",
    "    fam2 = random.choice(list(superfamily_to_items[sf2].keys()))\n",
    "    s1 = random.choice(superfamily_to_items[sf1][fam1])\n",
    "    s2 = random.choice(superfamily_to_items[sf2][fam2])\n",
    "    neg_pairs.append({\n",
    "        \"sentence1\": s1,\n",
    "        \"sentence2\": s2,\n",
    "        \"label\": 0\n",
    "    })\n",
    "    count += 1\n",
    "\n",
    "print(f\"负样本数量: {len(neg_pairs)}\")\n",
    "\n",
    "# ==================== 步骤5：合并、打乱、保存 ====================\n",
    "all_pairs = pos_pairs + neg_pairs\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "df = pd.DataFrame(all_pairs)\n",
    "\n",
    "print(f\"最终远程同源测试集规模: {len(df)} 条\")\n",
    "print(f\"  正样本 (同超家族不同家族): {len(df[df['label']==1])}\")\n",
    "print(f\"  负样本 (不同超家族): {len(df[df['label']==0])}\")\n",
    "\n",
    "# 保存\n",
    "df.to_csv(\"protein_pair_remote.csv\", index=False)\n",
    "print(\"测试集已保存为 protein_pair_remote.csv\")\n",
    "\n",
    "# 可选：上传到你的 biopaws 数据集\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"your_token\")\n",
    "# from datasets import Dataset\n",
    "# hf_ds = Dataset.from_pandas(df)\n",
    "# hf_ds.push_to_hub(\"dnagpt/biopaws\", config_name=\"remote-homology-scop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122bd364-d920-4e59-a41c-25593fc502fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8cded3e47941ca88819ecb3192933b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694e44947a894a7e8d69e290a575c33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/360 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f1c5dde84843b190f4b06e2a246c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aca8ef2aaa47af8401eddb3e5280de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f437286eee11491b894b6dec0720ca0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :   7%|7         | 6.29MB / 88.8MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dnagpt/biopaws/commit/6404c36e42e9f7b83cadcdd70029042757eb8b57', commit_message='Upload dataset', commit_description='', oid='6404c36e42e9f7b83cadcdd70029042757eb8b57', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/dnagpt/biopaws', endpoint='https://huggingface.co', repo_type='dataset', repo_id='dnagpt/biopaws'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可选：上传到你的 biopaws 数据集\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "from datasets import Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "hf_dataset.push_to_hub(\"dnagpt/biopaws\", config_name=\"protein_pair_remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15916b5-8a77-4d10-8feb-a6a279197209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
